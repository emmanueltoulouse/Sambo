using GLib;

namespace Sambo {
    /**
     * Gestionnaire de mod√®les llama.cpp
     * G√®re le chargement, l'initialisation et la lib√©ration des mod√®les
     */
    public class ModelManager : Object {
        private static ModelManager? instance = null;
        private string current_model_path = "";
        private bool is_model_loaded = false;
        private bool is_backend_initialized = false;
        private bool is_simulation_mode = false; // Mode r√©el par d√©faut
        private bool is_generation_cancelled = false; // Pour annuler la g√©n√©ration
        private Thread<void*>? current_generation_thread = null; // Thread actuel

        // Signaux pour informer l'interface
        public signal void model_loaded(string model_path, string model_name);
        public signal void model_load_failed(string model_path, string error_message);
        public signal void model_unloaded();
        public signal void generation_cancelled(); // Signal d'annulation

        /**
         * Singleton - obtenir l'instance unique
         */
        public static ModelManager get_instance() {
            if (instance == null) {
                instance = new ModelManager();
            }
            return instance;
        }

        /**
         * Constructeur public
         */
        public ModelManager() {
            // Initialiser le backend llama.cpp
            init_backend();
        }

        /**
         * Initialise le backend llama.cpp
         */
        private void init_backend() {
            if (!is_backend_initialized) {
                stderr.printf("üîß MODELMANAGER: Initialisation du backend llama.cpp...\n");
                try {
                    // Tentative d'initialisation r√©elle du backend llama.cpp via wrapper
                    bool success = Llama.backend_init();
                    if (success) {
                        is_backend_initialized = true;
                        is_simulation_mode = false;
                        stderr.printf("‚úÖ MODELMANAGER: Backend llama.cpp initialis√© avec succ√®s\n");
                    } else {
                        throw new IOError.NOT_FOUND("Backend llama.cpp non disponible");
                    }
                } catch (Error e) {
                    stderr.printf("‚ö†Ô∏è MODELMANAGER: llama.cpp non disponible, mode simulation activ√©: %s\n", e.message);
                    is_simulation_mode = true;
                    is_backend_initialized = false;
                }
            }
        }

        /**
         * Charge un mod√®le depuis un fichier
         * @param model_path Chemin vers le fichier du mod√®le
         * @return true si le chargement a r√©ussi, false sinon
         */
        public bool load_model(string model_path) {
            // V√©rifier que le fichier existe
            if (!FileUtils.test(model_path, FileTest.EXISTS)) {
                string error_msg = @"Le fichier mod√®le n'existe pas : $model_path";
                warning(error_msg);
                model_load_failed(model_path, error_msg);
                return false;
            }

            // Lib√©rer le mod√®le pr√©c√©dent s'il existe
            unload_current_model();

            stderr.printf("üìÇ MODELMANAGER: Chargement du mod√®le : %s\n", model_path);

            // Toujours tenter de charger via le wrapper C (qui g√®re le mode simulation)
            try {
                // Tentative de chargement du mod√®le via wrapper
                bool success = Llama.load_model(model_path);

                if (!success) {
                    throw new IOError.FAILED("√âchec du chargement du mod√®le");
                }

                // Succ√®s du chargement
                current_model_path = model_path;
                is_model_loaded = true;

                string model_name = Path.get_basename(model_path);
                if (is_simulation_mode) {
                    print("Mod√®le charg√© avec succ√®s en simulation : %s\n", model_name);
                } else {
                    print("Mod√®le charg√© avec succ√®s : %s\n", model_name);
                }

                model_loaded(model_path, model_name);
                return true;

            } catch (Error e) {
                // En cas d'erreur, essayer le mode simulation legacy
                warning("Erreur lors du chargement via wrapper, tentative simulation legacy : %s", e.message);
                return load_model_simulation(model_path);
            }
        }

        /**
         * Charge un mod√®le en mode simulation (pour les tests sans llama.cpp)
         */
        private bool load_model_simulation(string model_path) {
            print("Mode simulation : chargement simul√© du mod√®le %s\n", model_path);

            // Simuler un d√©lai de chargement
            Thread.usleep(500000); // 0.5 seconde

            // V√©rifier la taille du fichier (simulation de validation)
            try {
                var file = File.new_for_path(model_path);
                var file_info = file.query_info(FileAttribute.STANDARD_SIZE, FileQueryInfoFlags.NONE);
                int64 file_size = file_info.get_size();

                if (file_size < 1024) { // Fichier trop petit
                    string error_msg = "Le fichier mod√®le semble trop petit ou corrompu";
                    model_load_failed(model_path, error_msg);
                    return false;
                }

                // Succ√®s de la simulation
                current_model_path = model_path;
                is_model_loaded = true;

                string model_name = Path.get_basename(model_path);
                print("Mod√®le simul√© charg√© avec succ√®s : %s\n", model_name);

                model_loaded(model_path, model_name);
                return true;

            } catch (Error e) {
                string error_msg = @"Erreur lors de la validation du mod√®le : $(e.message)";
                model_load_failed(model_path, error_msg);
                return false;
            }
        }

        /**
         * D√©charge le mod√®le actuel
         */
        public void unload_current_model() {
            if (is_model_loaded) {
                print("D√©chargement du mod√®le actuel...\n");

                if (!is_simulation_mode) {
                    // Lib√©ration des ressources llama.cpp via wrapper
                    Llama.unload_model();
                }

                current_model_path = "";
                is_model_loaded = false;

                model_unloaded();
                print("Mod√®le d√©charg√©\n");
            }
        }

        /**
         * V√©rifie si un mod√®le est actuellement charg√©
         */
        public bool is_model_ready() {
            if (is_simulation_mode) {
                return is_model_loaded;
            }
            return is_model_loaded && Llama.is_model_loaded();
        }

        /**
         * Indique si le gestionnaire fonctionne en mode simulation
         */
        public bool is_in_simulation_mode() {
            return is_simulation_mode;
        }

        /**
         * Obtient le chemin du mod√®le actuellement charg√©
         */
        public string get_current_model_path() {
            return current_model_path;
        }

        /**
         * Obtient le nom du mod√®le actuellement charg√©
         */
        public string get_current_model_name() {
            if (current_model_path == "") {
                return "";
            }
            return Path.get_basename(current_model_path);
        }

        /**
         * G√©n√®re une r√©ponse IA en utilisant les param√®tres fournis
         * @param prompt Le prompt complet avec contexte
         * @param params Les param√®tres de sampling
         * @param callback Fonction appel√©e pour chaque token g√©n√©r√© (pour le streaming)
         * @return La r√©ponse compl√®te ou null en cas d'erreur
         */
        public string? generate_response(string prompt, Llama.SamplingParams params, owned GenerationCallback? callback = null) {
            if (!is_model_loaded) {
                stderr.printf("‚ùå MODELMANAGER: Aucun mod√®le charg√© pour la g√©n√©ration\n");
                return null;
            }

            // Annuler toute g√©n√©ration en cours
            if (is_generating()) {
                cancel_generation();
                // Attendre un peu que le thread pr√©c√©dent se termine
                Thread.usleep(100000); // 100ms
            }

            // R√©initialiser le flag d'annulation
            is_generation_cancelled = false;

            if (is_simulation_mode) {
                return generate_simulated_response(prompt, params, (owned) callback);
            }

            // G√©n√©ration asynchrone pour √©viter de bloquer l'UI
            generate_response_async.begin(prompt, params, (owned) callback, (obj, res) => {
                // Nettoyer la r√©f√©rence du thread une fois termin√©
                current_generation_thread = null;
            });
            
            return null; // La r√©ponse sera fournie via le callback
        }

        /**
         * G√©n√©ration asynchrone pour √©viter de bloquer l'interface utilisateur
         */
        private async void generate_response_async(string prompt, Llama.SamplingParams params, owned GenerationCallback? callback) {
            // Cr√©er un thread pour la g√©n√©ration
            current_generation_thread = new Thread<void*>("ai_generation", () => {
                stderr.printf("D√©marrage de la g√©n√©ration avec llama.cpp...\n");
                
                // V√©rifier l'annulation avant de commencer
                if (is_generation_cancelled) {
                    Idle.add(() => {
                        if (callback != null) {
                            callback("‚èπÔ∏è G√©n√©ration annul√©e", true);
                        }
                        return false;
                    });
                    return null;
                }
                
                // Estimation approximative du nombre de tokens (1 token ‚âà 4 caract√®res)
                var estimated_tokens = prompt.length / 4;
                stderr.printf("Prompt estim√© : %d tokens\n", estimated_tokens);
                
                // V√©rifier l'annulation avant la g√©n√©ration
                if (is_generation_cancelled) {
                    Idle.add(() => {
                        if (callback != null) {
                            callback("‚èπÔ∏è G√©n√©ration annul√©e", true);
                        }
                        return false;
                    });
                    return null;
                }
                
                // G√©n√©ration r√©elle via llama.cpp
                string? response = Llama.generate_simple(prompt, &params);
                
                // V√©rifier l'annulation apr√®s la g√©n√©ration
                if (is_generation_cancelled) {
                    Idle.add(() => {
                        if (callback != null) {
                            callback("‚èπÔ∏è G√©n√©ration annul√©e", true);
                        }
                        return false;
                    });
                    return null;
                }
                
                if (response != null && response.length > 0) {
                    stderr.printf("G√©n√©ration termin√©e avec succ√®s\n");
                    // Appeler le callback dans le thread principal
                    Idle.add(() => {
                        if (callback != null) {
                            callback(response, true); // true = termin√©
                        }
                        return false;
                    });
                } else {
                    stderr.printf("‚ùå MODELMANAGER: G√©n√©ration √©chou√©e ou r√©ponse vide\n");
                    Idle.add(() => {
                        if (callback != null) {
                            callback("‚ùå Erreur lors de la g√©n√©ration", true);
                        }
                        return false;
                    });
                }
                
                return null;
            });
            
            // Attendre la fin du thread de mani√®re asynchrone
            yield;
        }

        /**
         * G√©n√®re une r√©ponse simul√©e pour les tests
         */
        private string generate_simulated_response(string prompt, Llama.SamplingParams params, owned GenerationCallback? callback) {
            string response = """R√©ponse simul√©e du mod√®le IA.

ü§ñ **Mod√®le** : %s
üå°Ô∏è **Temp√©rature** : %.2f
üéØ **Top-P** : %.2f
üî¢ **Top-K** : %d
üìù **Max tokens** : %d

Votre message : "%s"

Cette r√©ponse est g√©n√©r√©e en mode simulation car llama.cpp n'est pas disponible ou aucun mod√®le n'est charg√©.
""".printf(
                get_current_model_name(), 
                params.temperature, 
                params.top_p, 
                params.top_k, 
                params.max_tokens,
                prompt.length > 100 ? prompt[0:100] + "..." : prompt
            );

            if (callback != null) {
                // Simuler le streaming progressif
                string[] words = response.split(" ");
                string partial = "";
                
                foreach (string word in words) {
                    partial += word + " ";
                    callback(partial, false);
                    Thread.usleep(50000); // 50ms de d√©lai pour simuler le streaming
                }
                
                callback(response, true); // Signal de fin
            }

            return response;
        }

        /**
         * Annule la g√©n√©ration en cours
         */
        public void cancel_generation() {
            is_generation_cancelled = true;
            generation_cancelled.emit();
            stderr.printf("üõë MODELMANAGER: G√©n√©ration annul√©e par l'utilisateur\n");
        }

        /**
         * V√©rifie si une g√©n√©ration est en cours
         */
        public bool is_generating() {
            return current_generation_thread != null;
        }

        /**
         * Type de d√©l√©gu√© pour les callbacks de g√©n√©ration (streaming)
         * @param partial_response R√©ponse partielle courante
         * @param is_finished true si la g√©n√©ration est termin√©e
         */
        public delegate void GenerationCallback(string partial_response, bool is_finished);

        /**
         * Destructeur - lib√®re les ressources
         */
        ~ModelManager() {
            unload_current_model();

            if (is_backend_initialized && !is_simulation_mode) {
                Llama.backend_free();
                is_backend_initialized = false;
            }
        }
    }
}
