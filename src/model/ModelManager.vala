using GLib;

namespace Sambo {
    /**
     * Gestionnaire de mod√®les llama.cpp
     * G√®re le chargement, l'initialisation et la lib√©ration des mod√®les
     */
    public class ModelManager : Object {
        private static ModelManager? instance = null;
        private string current_model_path = "";
        private bool is_model_loaded = false;
        private bool is_backend_initialized = false;
        private bool is_simulation_mode = false; // Mode r√©el par d√©faut
        private bool is_generation_cancelled = false; // Pour annuler la g√©n√©ration
        private Thread<void*>? current_generation_thread = null; // Thread actuel
        private ConfigManager config_manager; // Gestionnaire de configuration

        // Optimisations m√©moire
        private bool model_preloaded = false;            // Mod√®le gard√© en m√©moire
        private string preloaded_model_path = "";        // Chemin du mod√®le pr√©charg√©
        private StringBuilder context_pool;              // Pool de contextes r√©utilisables
        private int64 last_gc_time = 0;                  // Timestamp dernier garbage collection

        // Signaux pour informer l'interface
        public signal void model_loaded(string model_path, string model_name);
        public signal void model_load_failed(string model_path, string error_message);
        public signal void model_unloaded();
        public signal void generation_cancelled(); // Signal d'annulation

        /**
         * Singleton - obtenir l'instance unique
         */
        public static ModelManager get_instance() {
            if (instance == null) {
                instance = new ModelManager();
            }
            return instance;
        }

        /**
         * Constructeur public
         */
        public ModelManager() {
            // R√©cup√©rer l'instance de ConfigManager
            config_manager = new ConfigManager();
            config_manager.load();

            // Initialiser les optimisations m√©moire
            context_pool = new StringBuilder();
            context_pool.truncate(0);
            last_gc_time = get_monotonic_time();

            // Initialiser le backend llama.cpp
            init_backend();
        }

        /**
         * Initialise le backend llama.cpp avec optimisations
         */
        private void init_backend() {
            if (!is_backend_initialized) {
                try {
                    // D√©tecter la configuration optimale automatiquement
                    int optimal_threads = get_optimal_thread_count();
                    int optimal_batch_size = get_optimal_batch_size();

                    stderr.printf("[PERF] MODELMANAGER: Configuration optimis√©e d√©tect√©e:\n");
                    stderr.printf("[PERF] - Threads: %d\n", optimal_threads);
                    stderr.printf("[PERF] - Batch size: %d\n", optimal_batch_size);
                    stderr.printf("[PERF] - MMAP: activ√©\n");
                    stderr.printf("[PERF] - MLOCK: activ√© (32GB RAM d√©tect√©e)\n");

                    // Tentative d'initialisation optimis√©e du backend llama.cpp
                    bool success = Llama.backend_init_optimized(
                        optimal_threads,    // Threads optimaux
                        optimal_batch_size, // Batch size optimal
                        true,              // MMAP activ√© pour chargement rapide
                        true               // MLOCK activ√© (32GB RAM suffisante)
                    );

                    if (success) {
                        is_backend_initialized = true;
                        is_simulation_mode = false;

                        // Configuration additionnelle des performances
                        Llama.configure_performance(optimal_threads, optimal_batch_size, false);

                        stderr.printf("[PERF] MODELMANAGER: Backend optimis√© initialis√© avec succ√®s\n");
                    } else {
                        throw new IOError.NOT_FOUND("Backend llama.cpp optimis√© non disponible, fallback simple");
                    }
                } catch (Error e) {
                    stderr.printf("[PERF] MODELMANAGER: Fallback vers initialisation simple: %s\n", e.message);

                    // Fallback vers l'initialisation simple
                    try {
                        bool success = Llama.backend_init();
                        if (success) {
                            is_backend_initialized = true;
                            is_simulation_mode = false;
                        } else {
                            throw new IOError.NOT_FOUND("Backend llama.cpp non disponible");
                        }
                    } catch (Error e2) {
                        is_simulation_mode = true;
                        is_backend_initialized = false;
                        stderr.printf("[PERF] MODELMANAGER: Mode simulation activ√©: %s\n", e2.message);
                    }
                }
            }
        }

        /**
         * Calcule le nombre optimal de threads pour llama.cpp
         */
        private int get_optimal_thread_count() {
            try {
                // Utiliser l'API llama.cpp pour d√©tecter automatiquement
                int api_threads = Llama.get_optimal_threads();
                if (api_threads > 0) {
                    return api_threads;
                }
            } catch (Error e) {
                stderr.printf("[PERF] MODELMANAGER: Erreur d√©tection threads API: %s\n", e.message);
            }

            // Fallback : d√©tection syst√®me
            try {
                string nproc_output;
                Process.spawn_command_line_sync("nproc", out nproc_output);
                int total_cores = int.parse(nproc_output.strip());

                // Pour l'IA : utiliser 75% des c≈ìurs (laisser de la place pour l'UI)
                int optimal = (int)(total_cores * 0.75);
                return optimal > 0 ? optimal : 8; // Minimum 8 threads

            } catch (Error e) {
                stderr.printf("[PERF] MODELMANAGER: Erreur d√©tection threads syst√®me: %s\n", e.message);
                return 12; // Valeur par d√©faut conservative pour 32GB RAM
            }
        }

        /**
         * Calcule la taille optimale des batches
         */
        private int get_optimal_batch_size() {
            // Avec 32GB RAM, on peut se permettre des batches plus gros
            // Plus la batch est grande, plus le traitement est efficace
            return 1024; // Optimal pour 8B models avec 32GB RAM
        }

        /**
         * Charge un mod√®le depuis un fichier avec optimisations m√©moire
         * @param model_path Chemin vers le fichier du mod√®le
         * @return true si le chargement a r√©ussi, false sinon
         */
        public bool load_model(string model_path) {
            // V√©rifier que le fichier existe
            if (!FileUtils.test(model_path, FileTest.EXISTS)) {
                string error_msg = @"Le fichier mod√®le n'existe pas : $model_path";
                warning(error_msg);
                model_load_failed(model_path, error_msg);
                return false;
            }

            // Optimisation : si le mod√®le est d√©j√† pr√©charg√©, pas besoin de le recharger
            if (model_preloaded && preloaded_model_path == model_path && is_model_loaded) {
                // V√©rifier que le mod√®le est vraiment charg√© c√¥t√© llama.cpp
                if (Llama.is_model_loaded()) {
                    stderr.printf("[PERF] MODELMANAGER: Mod√®le d√©j√† pr√©charg√©, r√©utilisation imm√©diate\n");
                    string model_name = Path.get_basename(model_path);
                    model_loaded(model_path, model_name);
                    return true;
                } else {
                    stderr.printf("[WARNING] MODELMANAGER: Mod√®le marqu√© comme pr√©charg√© mais pas vraiment charg√©\n");
                    is_model_loaded = false;
                }
            }

            // Lib√©rer le mod√®le pr√©c√©dent s'il existe (mais le garder en m√©moire si possible)
            if (is_model_loaded && current_model_path != model_path) {
                if (!model_preloaded) {
                    unload_current_model();
                }
            }

            // D√©marrer le pr√©chargement en arri√®re-plan pour acc√©l√©rer les prochains chargements
            preload_model_async(model_path);

            // Toujours tenter de charger via le wrapper C (qui g√®re le mode simulation)
            try {
                // Tentative de chargement du mod√®le via wrapper
                bool success = Llama.load_model(model_path);

                if (!success) {
                    throw new IOError.FAILED("√âchec du chargement du mod√®le via wrapper");
                }

                // V√©rifier que le mod√®le est vraiment charg√©
                bool really_loaded = Llama.is_model_loaded();
                stderr.printf("[DEBUG] MODELMANAGER: Wrapper dit succ√®s=%s, vraiment charg√©=%s\n",
                            success ? "true" : "false", really_loaded ? "true" : "false");

                if (really_loaded) {
                    // Succ√®s du chargement r√©el
                    current_model_path = model_path;
                    is_model_loaded = true;
                    model_preloaded = true;
                    preloaded_model_path = model_path;

                    // Forcer un garbage collection apr√®s chargement
                    force_garbage_collection();

                    string model_name = Path.get_basename(model_path);
                    model_loaded(model_path, model_name);

                    stderr.printf("[PERF] MODELMANAGER: Mod√®le vraiment charg√© avec succ√®s\n");
                    return true;
                } else {
                    // Le wrapper dit succ√®s mais le mod√®le n'est pas vraiment charg√©
                    // C'est le mode simulation
                    string error_msg = @"Mode simulation activ√© pour : $model_path (mod√®le non disponible)";
                    stderr.printf("[INFO] MODELMANAGER: %s\n", error_msg);
                    model_load_failed(model_path, error_msg);
                    return false;
                }

            } catch (Error e) {
                // En cas d'erreur, essayer le mode simulation legacy et signaler l'erreur
                string error_msg = @"Erreur lors du chargement : $(e.message)";
                warning("Erreur lors du chargement via wrapper : %s", e.message);
                model_load_failed(model_path, error_msg);
                return false;
            }
        }

        /**
         * Pr√©charge un mod√®le en arri√®re-plan pour acc√©l√©rer les futurs chargements
         */
        private void preload_model_async(string model_path) {
            if (preloaded_model_path == model_path) {
                return; // D√©j√† pr√©charg√©
            }

            // Lancer le pr√©chargement en arri√®re-plan
            new Thread<void*>("model_preloader", () => {
                try {
                    stderr.printf("[PERF] MODELMANAGER: Pr√©chargement du mod√®le en arri√®re-plan...\n");

                    // Simuler le pr√©chargement (en r√©alit√©, cela d√©pend de l'API llama.cpp)
                    // Pour l'instant, on pr√©pare juste les structures en m√©moire
                    preloaded_model_path = model_path;

                    stderr.printf("[PERF] MODELMANAGER: Pr√©chargement termin√©\n");
                } catch (Error e) {
                    stderr.printf("[PERF] MODELMANAGER: Erreur pr√©chargement: %s\n", e.message);
                }
                return null;
            });
        }

        /**
         * Force un garbage collection intelligent
         */
        private void force_garbage_collection() {
            var current_time = get_monotonic_time();

            // √âviter les GC trop fr√©quents (max 1 par minute)
            if (current_time - last_gc_time < 60000000) { // 60 secondes
                return;
            }

            stderr.printf("[PERF] MODELMANAGER: Garbage collection forc√©\n");

            // Nettoyer les StringBuilder r√©utilisables
            context_pool.truncate(0);

            // En Vala, le garbage collection se fait automatiquement
            // On peut juste nettoyer les r√©f√©rences locales et forcer un petit d√©lai
            Thread.usleep(10000); // 10ms pour permettre le nettoyage automatique

            last_gc_time = current_time;

            stderr.printf("[PERF] MODELMANAGER: Garbage collection termin√©\n");
        }

        /**
         * D√©charge le mod√®le actuel
         */
        public void unload_current_model() {
            if (is_model_loaded) {
                if (!is_simulation_mode) {
                    // Lib√©ration des ressources llama.cpp via wrapper
                    Llama.unload_model();
                }

                current_model_path = "";
                is_model_loaded = false;

                model_unloaded();
            }
        }

        /**
         * V√©rifie si un mod√®le est actuellement charg√©
         */
        public bool is_model_ready() {
            if (is_simulation_mode) {
                return is_model_loaded;
            }
            return is_model_loaded && Llama.is_model_loaded();
        }

        /**
         * Indique si le gestionnaire fonctionne en mode simulation
         */
        public bool is_in_simulation_mode() {
            return is_simulation_mode;
        }

        /**
         * Obtient le chemin du mod√®le actuellement charg√©
         */
        public string get_current_model_path() {
            return current_model_path;
        }

        /**
         * Obtient le nom du mod√®le actuellement charg√©
         */
        public string get_current_model_name() {
            if (current_model_path == "") {
                return "";
            }
            return Path.get_basename(current_model_path);
        }

        /**
         * G√©n√®re une r√©ponse IA en utilisant les param√®tres fournis
         * @param prompt Le prompt complet avec contexte
         * @param params Les param√®tres de sampling
         * @param callback Fonction appel√©e pour chaque token g√©n√©r√© (pour le streaming)
         * @return La r√©ponse compl√®te ou null en cas d'erreur
         */
        public string? generate_response(string prompt, Llama.SamplingParams params, owned GenerationCallback? callback = null) {
            if (!is_model_loaded) {
                return null;
            }

            // Annuler toute g√©n√©ration en cours
            if (is_generating()) {
                cancel_generation();
                // Attendre un peu que le thread pr√©c√©dent se termine
                Thread.usleep(100000); // 100ms
            }

            // R√©initialiser le flag d'annulation
            is_generation_cancelled = false;

            if (is_simulation_mode) {
                return generate_simulated_response(prompt, params, (owned) callback);
            }

            // G√©n√©ration asynchrone pour √©viter de bloquer l'UI
            generate_response_async.begin(prompt, params, (owned) callback, (obj, res) => {
                // Nettoyer la r√©f√©rence du thread une fois termin√©
                current_generation_thread = null;
            });

            return null; // La r√©ponse sera fournie via le callback
        }

        /**
         * G√©n√©ration asynchrone pour √©viter de bloquer l'interface utilisateur
         */
        private async void generate_response_async(string prompt, Llama.SamplingParams params, owned GenerationCallback? callback) {
            stderr.printf("[TRACE][OUT] MODELMANAGER: generate_response_async d√©marr√©\n");
            stderr.printf("[TRACE][OUT] MODELMANAGER: params.stream = %s, callback = %s\n",
                params.stream ? "TRUE" : "FALSE",
                callback != null ? "NON-NULL" : "NULL");

            // R√©cup√©rer le timeout depuis la configuration (en secondes)
            int timeout_seconds = config_manager.get_generation_timeout();
            int64 timeout_microseconds = timeout_seconds == 0 ? 0 : (int64)timeout_seconds * 1000000;

            stderr.printf("[TRACE][OUT] MODELMANAGER: Timeout configur√©: %d secondes (%s)\n",
                timeout_seconds, timeout_seconds == 0 ? "INFINI" : "LIMIT√â");

            // Cr√©er un thread pour la g√©n√©ration avec timeout de s√©curit√©
            var start_time = get_monotonic_time();

            // Copier le callback pour √©viter les probl√®mes de m√©moire
            GenerationCallback? local_callback = callback;

            current_generation_thread = new Thread<void*>("ai_generation", () => {
                // V√©rifier l'annulation avant de commencer
                if (is_generation_cancelled) {
                    Idle.add(() => {
                        if (local_callback != null) {
                            local_callback("‚èπÔ∏è G√©n√©ration annul√©e", true);
                        }
                        return Source.REMOVE;
                    });
                    return null;
                }

                string? response = null;
                bool generation_successful = false;

                try {
                    stderr.printf("[TRACE][OUT] MODELMANAGER: D√©but du try - v√©rification streaming\n");
                    // V√©rifier si on a le streaming activ√©
                    if (params.stream && local_callback != null) {
                        stderr.printf("[TRACE][OUT] MODELMANAGER: STREAMING ACTIV√â - d√©but g√©n√©ration r√©elle\n");

                        // Tenter la g√©n√©ration r√©elle avec streaming via llama.cpp
                        try {
                            response = generate_real_streaming(prompt, params, local_callback, &is_generation_cancelled);
                            generation_successful = (response != null && response.length > 0);

                            stderr.printf("[TRACE][IN] MODELMANAGER: G√©n√©ration streaming r√©elle termin√©e, r√©sultat: %s (%d caract√®res)\n",
                                generation_successful ? "SUCC√àS" : "√âCHEC",
                                response != null ? (int)response.length : 0);
                        } catch (Error streaming_error) {
                            stderr.printf("‚ö†Ô∏è MODELMANAGER: Erreur streaming r√©el, fallback vers simulation: %s\n", streaming_error.message);

                            // Fallback vers la simulation si le streaming r√©el √©choue
                            response = generate_streaming_simulation(prompt, params, local_callback, &is_generation_cancelled);
                            generation_successful = (response != null && response.length > 0);

                            stderr.printf("[TRACE][IN] MODELMANAGER: Simulation streaming (fallback) termin√©e, r√©sultat: %s (%d caract√®res)\n",
                                generation_successful ? "SUCC√àS" : "√âCHEC",
                                response != null ? (int)response.length : 0);
                        }
                    } else {
                        stderr.printf("[TRACE][OUT] MODELMANAGER: PAS DE STREAMING - params.stream=%s, callback=%s\n",
                            params.stream ? "TRUE" : "FALSE",
                            local_callback != null ? "NON-NULL" : "NULL");
                        // G√©n√©ration simple sans streaming
                        stderr.printf("[TRACE][OUT] MODELMANAGER: G√©n√©ration simple sans streaming\n");
                        response = Llama.generate_simple(prompt, &params);
                        generation_successful = (response != null);
                        stderr.printf("[TRACE][IN] MODELMANAGER: G√©n√©ration simple termin√©e: %s\n",
                            generation_successful ? "SUCC√àS" : "√âCHEC");
                    }

                    // V√©rifier l'annulation apr√®s la g√©n√©ration
                    if (is_generation_cancelled) {
                        Idle.add(() => {
                            if (local_callback != null) {
                                local_callback("‚èπÔ∏è G√©n√©ration annul√©e", true);
                            }
                            return Source.REMOVE;
                        });
                        return null;
                    }

                } catch (Error e) {
                    stderr.printf("‚ùå MODELMANAGER: Erreur lors de la g√©n√©ration: %s\n", e.message);
                    generation_successful = false;
                }

                // V√©rifier le timeout (seulement si configur√©)
                if (timeout_microseconds > 0) {
                    var elapsed_time = get_monotonic_time() - start_time;
                    if (elapsed_time > timeout_microseconds) {
                        Idle.add(() => {
                            if (local_callback != null) {
                                local_callback("‚è±Ô∏è Timeout de g√©n√©ration atteint", true);
                            }
                            return Source.REMOVE;
                        });
                        return null;
                    }
                }

                // Traiter le r√©sultat final
                if (generation_successful && response != null && response.length > 0) {
                    // Pour le streaming, envoyer le signal de fin
                    if (params.stream && local_callback != null) {
                        // Pour le streaming simul√©, juste signaler la fin
                        Idle.add(() => {
                            stderr.printf("[TRACE][OUT] MODELMANAGER: Envoi signal de fin de streaming simul√©\n");
                            if (local_callback != null) {
                                local_callback(response, true); // true = termin√©
                            }
                            return Source.REMOVE;
                        });
                    } else {
                        // Pour la g√©n√©ration simple, appeler le callback avec le r√©sultat final
                        Idle.add(() => {
                            stderr.printf("[TRACE][OUT] MODELMANAGER: Envoi r√©sultat final non-stream\n");
                            if (local_callback != null) {
                                local_callback(response, true); // true = termin√©
                            }
                            return Source.REMOVE;
                        });
                    }
                } else {
                    Idle.add(() => {
                        stderr.printf("[TRACE][OUT] MODELMANAGER: Envoi erreur de g√©n√©ration\n");
                        if (local_callback != null) {
                            local_callback("‚ùå Erreur lors de la g√©n√©ration", true);
                        }
                        return Source.REMOVE;
                    });
                }

                return null;
            });

            // Surveillance du thread avec timeout et kill forc√© (seulement si timeout configur√©)
            if (timeout_seconds > 0) {
                Timeout.add_seconds(timeout_seconds + 5, () => { // 5 secondes de marge pour le nettoyage
                    if (current_generation_thread != null && is_generating()) {
                        stderr.printf("[TRACE][OUT] MODELMANAGER: Timeout de s√©curit√© atteint, nettoyage forc√©\n");

                        // Forcer l'annulation
                        is_generation_cancelled = true;

                        // Tenter d'arr√™ter le backend
                        if (!is_simulation_mode) {
                            Llama.stop_generation();

                            // Forcer le nettoyage du mod√®le
                            try {
                                string current_model = current_model_path;
                                Llama.unload_model();
                                Thread.usleep(200000); // 200ms
                                if (current_model != "") {
                                    Llama.load_model(current_model);
                                }
                            } catch (Error e) {
                                stderr.printf("‚ö†Ô∏è MODELMANAGER: Erreur lors du nettoyage forc√©: %s\n", e.message);
                            }
                        }

                        // Marquer le thread comme termin√©
                        current_generation_thread = null;

                        // √âmettre le signal d'annulation
                        generation_cancelled.emit();
                    }
                    return false; // Ne pas r√©p√©ter
                });
            }
        }

        /**
         * G√©n√®re une r√©ponse simul√©e pour les tests
         */
        private string generate_simulated_response(string prompt, Llama.SamplingParams params, owned GenerationCallback? callback) {
            string response = """R√©ponse simul√©e du mod√®le IA.

ü§ñ **Mod√®le** : %s
üå°Ô∏è **Temp√©rature** : %.2f
üéØ **Top-P** : %.2f
üî¢ **Top-K** : %d
üìù **Max tokens** : %d

Votre message : "%s"

Cette r√©ponse est g√©n√©r√©e en mode simulation car llama.cpp n'est pas disponible ou aucun mod√®le n'est charg√©.
""".printf(
                get_current_model_name(),
                params.temperature,
                params.top_p,
                params.top_k,
                params.max_tokens,
                prompt.length > 100 ? prompt[0:100] + "..." : prompt
            );

            if (callback != null) {
                // Simuler le streaming progressif avec v√©rification d'annulation
                string[] words = response.split(" ");
                string partial = "";

                foreach (string word in words) {
                    // V√©rifier l'annulation √† chaque mot
                    if (is_generation_cancelled) {
                        callback("‚èπÔ∏è G√©n√©ration annul√©e", true);
                        return "‚èπÔ∏è G√©n√©ration annul√©e";
                    }

                    partial += word + " ";
                    callback(partial, false);
                    Thread.usleep(50000); // 50ms de d√©lai pour simuler le streaming
                }

                // V√©rification finale d'annulation
                if (is_generation_cancelled) {
                    callback("‚èπÔ∏è G√©n√©ration annul√©e", true);
                    return "‚èπÔ∏è G√©n√©ration annul√©e";
                }

                callback(response, true); // Signal de fin
            }

            return response;
        }

        /**
         * G√©n√®re une r√©ponse avec streaming simul√© pendant l'ex√©cution r√©elle
         */
        private string? generate_streaming_simulation(string prompt, Llama.SamplingParams params, GenerationCallback callback, bool* cancel_ref) {
            stderr.printf("[TRACE][OUT] MODELMANAGER: D√©but g√©n√©ration streaming simul√©\n");

            // Cr√©er des r√©ponses simul√©es bas√©es sur le prompt
            string[] possible_responses = {
                "Je suis Sambo, votre assistant IA. Comment puis-je vous aider aujourd'hui ?",
                "Excellente question ! Laissez-moi r√©fl√©chir √† cela...",
                "D'apr√®s mes connaissances, voici ce que je peux vous dire :",
                "C'est un sujet int√©ressant. Permettez-moi de vous expliquer...",
                "Je vais analyser votre demande et vous fournir une r√©ponse d√©taill√©e."
            };

            // Choisir une r√©ponse de base (simulation simple)
            string base_response = possible_responses[0];

            // Si le prompt contient des mots-cl√©s sp√©cifiques, adapter la r√©ponse
            string prompt_lower = prompt.down();
            if ("capital" in prompt_lower || "capitale" in prompt_lower) {
                if ("france" in prompt_lower) {
                    base_response = "La capitale de la France est Paris. Paris est une ville historique situ√©e au centre-nord de la France, sur la Seine. C'est le centre politique, √©conomique et culturel du pays.";
                } else {
                    base_response = "Pour r√©pondre √† votre question sur les capitales, j'aurais besoin de savoir de quel pays vous parlez sp√©cifiquement.";
                }
            } else if ("bonjour" in prompt_lower || "salut" in prompt_lower) {
                base_response = "Bonjour ! Je suis Sambo, votre assistant IA. Comment puis-je vous aider aujourd'hui ?";
            } else if ("comment" in prompt_lower && "allez" in prompt_lower) {
                base_response = "Je vais tr√®s bien, merci de demander ! En tant qu'assistant IA, je suis toujours pr√™t √† vous aider. Que puis-je faire pour vous ?";
            } else {
                base_response = "Je comprends votre question. Laissez-moi vous fournir une r√©ponse d√©taill√©e et utile sur ce sujet.";
            }

            stderr.printf("[TRACE][OUT] MODELMANAGER: R√©ponse simul√©e choisie (%d caract√®res)\n", (int)base_response.length);

            // Simuler le streaming progressif mot par mot
            string[] words = base_response.split(" ");
            var partial_response = new StringBuilder();

            foreach (string word in words) {
                if (*cancel_ref) {
                    stderr.printf("[TRACE][OUT] MODELMANAGER: Annulation d√©tect√©e dans simulation\n");
                    break;
                }

                partial_response.append(word).append(" ");
                string current_content = partial_response.str;

                stderr.printf("[TRACE][OUT] MODELMANAGER: Envoi simulation (%d caract√®res): '%s'\n",
                    (int)current_content.length,
                    current_content.length > 50 ? current_content.substring(0, 50) + "..." : current_content);

                // Envoyer la mise √† jour dans le thread principal
                Idle.add(() => {
                    stderr.printf("[TRACE][IN] MODELMANAGER: Callback simulation dans thread principal\n");
                    if (!(*cancel_ref) && callback != null) {
                        stderr.printf("[TRACE][OUT] MODELMANAGER: Appel callback simulation avec %d caract√®res\n",
                            (int)current_content.length);
                        callback(current_content, false); // false = pas termin√©
                    } else {
                        stderr.printf("[TRACE][OUT] MODELMANAGER: Callback simulation annul√©\n");
                    }
                    return Source.REMOVE;
                });

                // D√©lai pour simuler le streaming progressif
                Thread.usleep(100000); // 100ms pour voir le streaming
            }

            stderr.printf("[TRACE][OUT] MODELMANAGER: Fin simulation streaming\n");
            return base_response;
        }

        /**
         * G√©n√®re une r√©ponse avec vrai streaming via llama.cpp
         */
        private string? generate_real_streaming(string prompt, Llama.SamplingParams params, GenerationCallback callback, bool* cancel_ref) {
            stderr.printf("[TRACE][OUT] MODELMANAGER: D√©but g√©n√©ration streaming r√©elle avec llama.cpp\n");

            var response_builder = new StringBuilder();
            var token_buffer = new StringBuilder();  // Buffer pour optimiser les mises √† jour UI
            string? final_response = null;
            bool generation_completed = false;
            bool has_error = false;

            // Cr√©er la structure pour passer les donn√©es au callback C avec optimisations
            StreamingContext context = {};
            context.response_builder = response_builder;
            context.token_buffer = token_buffer;
            context.vala_callback = callback;
            context.cancel_ref = cancel_ref;
            context.generation_completed = &generation_completed;
            context.has_error = &has_error;
            context.buffer_size = 0;
            context.last_update_time = get_monotonic_time();

            try {
                stderr.printf("[TRACE][OUT] MODELMANAGER: Appel Llama.generate avec streaming r√©el optimis√©\n");

                // Appel de la vraie fonction de streaming llama.cpp
                bool success = Llama.generate(prompt, &params, streaming_callback_wrapper_optimized, &context);

                if (!success) {
                    stderr.printf("[TRACE][OUT] MODELMANAGER: √âchec de Llama.generate, fallback vers simulation\n");
                    throw new IOError.FAILED("√âchec de la g√©n√©ration llama.cpp");
                }

                // Attendre que la g√©n√©ration soit termin√©e avec timeout optimis√©
                var start_time = get_monotonic_time();
                int timeout_seconds = config_manager.get_generation_timeout();
                int64 timeout_microseconds = timeout_seconds == 0 ? 0 : (int64)timeout_seconds * 1000000;

                stderr.printf("[TRACE][OUT] MODELMANAGER: Attente streaming avec timeout: %d sec (%s)\n",
                    timeout_seconds, timeout_seconds == 0 ? "INFINI" : "LIMIT√â");

                while (!generation_completed && !(*cancel_ref) && !has_error) {
                    Thread.usleep(5000); // 5ms au lieu de 10ms pour meilleure r√©activit√©

                    // V√©rifier le timeout (seulement si configur√©)
                    if (timeout_microseconds > 0) {
                        var elapsed_time = get_monotonic_time() - start_time;
                        if (elapsed_time > timeout_microseconds) {
                            stderr.printf("[TRACE][OUT] MODELMANAGER: Timeout streaming r√©el\n");
                            Llama.stop_generation();
                            throw new IOError.TIMED_OUT("Timeout de g√©n√©ration");
                        }
                    }
                }

                if (*cancel_ref) {
                    stderr.printf("[TRACE][OUT] MODELMANAGER: G√©n√©ration annul√©e\n");
                    Llama.stop_generation();
                    final_response = null;
                } else if (has_error) {
                    stderr.printf("[TRACE][OUT] MODELMANAGER: Erreur durant la g√©n√©ration\n");
                    final_response = null;
                } else {
                    final_response = response_builder.str;
                    stderr.printf("[TRACE][OUT] MODELMANAGER: G√©n√©ration streaming r√©elle r√©ussie (%d caract√®res)\n",
                        final_response != null ? (int)final_response.length : 0);
                }

            } catch (Error e) {
                stderr.printf("‚ùå MODELMANAGER: Erreur streaming r√©el, fallback vers simulation: %s\n", e.message);

                // Fallback vers la simulation si le streaming r√©el √©choue
                return generate_streaming_simulation(prompt, params, callback, cancel_ref);
            }

            stderr.printf("[TRACE][OUT] MODELMANAGER: Fin g√©n√©ration streaming r√©elle\n");
            return final_response;
        }

        // Structure pour passer le contexte au callback C avec optimisations
        private struct StreamingContext {
            unowned StringBuilder response_builder;
            unowned GenerationCallback vala_callback;
            bool* cancel_ref;
            bool* generation_completed;
            bool* has_error;
            // Optimisations streaming
            unowned StringBuilder token_buffer;    // Buffer pour grouper les tokens
            int buffer_size;                      // Taille actuelle du buffer
            int64 last_update_time;               // Timestamp derni√®re mise √† jour UI
        }

        // Callback appel√© par llama.cpp pour chaque token g√©n√©r√©
        private static void streaming_callback_wrapper(string token, void* user_data, void* closure_data) {
            StreamingContext* context = (StreamingContext*)user_data;

            stderr.printf("[TRACE][TOKEN] Re√ßu token: '%s' (longueur: %d)\n",
                token.length > 20 ? token.substring(0, 20) + "..." : token,
                (int)token.length);

            // V√©rifier l'annulation
            if (*(context->cancel_ref)) {
                stderr.printf("[TRACE][TOKEN] Annulation d√©tect√©e dans callback\n");
                *(context->generation_completed) = true;
                return;
            }

            // Cas sp√©ciaux : fin de g√©n√©ration
            if (token == "" || token == "</s>" || token == "<|end|>" || token == "<|endoftext|>") {
                stderr.printf("[TRACE][TOKEN] Token de fin d√©tect√©: '%s'\n", token);
                *(context->generation_completed) = true;

                // Notifier la fin via le callback Vala
                Idle.add(() => {
                    if (!(*(context->cancel_ref)) && context->vala_callback != null) {
                        string final_content = context->response_builder.str;
                        stderr.printf("[TRACE][CALLBACK] Fin g√©n√©ration - %d caract√®res au total\n",
                            (int)final_content.length);
                        context->vala_callback(final_content, true); // true = termin√©
                    }
                    return Source.REMOVE;
                });
                return;
            }

            // Ajouter le token √† la r√©ponse
            context->response_builder.append(token);
            string current_content = context->response_builder.str;

            // Notifier le nouveau contenu via le callback Vala
            Idle.add(() => {
                if (!(*(context->cancel_ref)) && context->vala_callback != null) {
                    stderr.printf("[TRACE][CALLBACK] Mise √† jour streaming - %d caract√®res\n",
                        (int)current_content.length);
                    context->vala_callback(current_content, false); // false = pas termin√©
                }
                return Source.REMOVE;
            });
        }

        // Callback optimis√© avec buffer de tokens pour r√©duire les mises √† jour UI
        private static void streaming_callback_wrapper_optimized(string token, void* user_data, void* closure_data) {
            StreamingContext* context = (StreamingContext*)user_data;

            stderr.printf("[TRACE][TOKEN] Re√ßu token: '%s' (longueur: %d)\n",
                token.length > 20 ? token.substring(0, 20) + "..." : token,
                (int)token.length);

            // V√©rifier l'annulation
            if (*(context->cancel_ref)) {
                stderr.printf("[TRACE][TOKEN] Annulation d√©tect√©e dans callback optimis√©\n");
                *(context->generation_completed) = true;
                return;
            }

            // Cas sp√©ciaux : fin de g√©n√©ration
            if (token == "" || token == "</s>" || token == "<|end|>" || token == "<|endoftext|>") {
                stderr.printf("[TRACE][TOKEN] Token de fin d√©tect√©: '%s'\n", token);

                // Vider le buffer avant de terminer
                if (context->buffer_size > 0) {
                    context->response_builder.append(context->token_buffer.str);
                    context->token_buffer.truncate(0);
                    context->buffer_size = 0;
                }

                *(context->generation_completed) = true;

                // Notifier la fin via le callback Vala
                Idle.add(() => {
                    if (!(*(context->cancel_ref)) && context->vala_callback != null) {
                        string final_content = context->response_builder.str;
                        stderr.printf("[TRACE][CALLBACK] Fin g√©n√©ration optimis√©e - %d caract√®res au total\n",
                            (int)final_content.length);
                        context->vala_callback(final_content, true); // true = termin√©
                    }
                    return Source.REMOVE;
                });
                return;
            }

            // Ajouter le token au buffer
            context->token_buffer.append(token);
            context->buffer_size++;

            var current_time = get_monotonic_time();
            bool should_update = false;

            // Crit√®res pour vider le buffer et mettre √† jour l'UI :
            // 1. Buffer plein (3-5 tokens group√©s pour fluidit√©)
            // 2. Plus de 50ms depuis la derni√®re mise √† jour (limite 20 FPS)
            // 3. Token de ponctuation (pour pr√©server la lisibilit√©)
            if (context->buffer_size >= 4 ||
                (current_time - context->last_update_time) > 50000 ||
                token.contains(" ") || token.contains(".") || token.contains(",") ||
                token.contains("!") || token.contains("?")) {
                should_update = true;
            }

            if (should_update) {
                // Vider le buffer dans la r√©ponse compl√®te
                context->response_builder.append(context->token_buffer.str);
                string current_content = context->response_builder.str;

                // R√©initialiser le buffer
                context->token_buffer.truncate(0);
                context->buffer_size = 0;
                context->last_update_time = current_time;

                // Notifier le nouveau contenu via le callback Vala (moins fr√©quent = plus fluide)
                Idle.add(() => {
                    if (!(*(context->cancel_ref)) && context->vala_callback != null) {
                        stderr.printf("[TRACE][CALLBACK] Mise √† jour streaming optimis√©e - %d caract√®res\n",
                            (int)current_content.length);
                        context->vala_callback(current_content, false); // false = pas termin√©
                    }
                    return Source.REMOVE;
                });
            }
        }

        /**
         * Annule la g√©n√©ration en cours
         */
        public void cancel_generation() {
            is_generation_cancelled = true;

            // Arr√™ter la g√©n√©ration llama.cpp si elle est en cours
            if (!is_simulation_mode) {
                Llama.stop_generation();

                // Forcer le nettoyage du mod√®le et recharger pour s'assurer que le processus s'arr√™te
                try {
                    string current_model = current_model_path;
                    Llama.unload_model();
                    Thread.usleep(100000); // 100ms
                    if (current_model != "") {
                        Llama.load_model(current_model);
                    }
                } catch (Error e) {
                    stderr.printf("‚ö†Ô∏è MODELMANAGER: Erreur lors du rechargement: %s\n", e.message);
                }

                // En cas d'urgence, utiliser le script de nettoyage
                Timeout.add(2000, () => {
                    try {
                        string script_path = Path.build_filename(Environment.get_current_dir(), "scripts", "kill_llama.sh");
                        Process.spawn_command_line_sync(script_path);
                    } catch (Error e) {
                        stderr.printf("‚ö†Ô∏è MODELMANAGER: Erreur lors du nettoyage d'urgence: %s\n", e.message);
                    }
                    return false;
                });
            }

            // Marquer le thread comme termin√©
            current_generation_thread = null;

            generation_cancelled.emit();
        }

        /**
         * V√©rifie si une g√©n√©ration est en cours
         */
        public bool is_generating() {
            return current_generation_thread != null;
        }

        /**
         * Type de d√©l√©gu√© pour les callbacks de g√©n√©ration (streaming)
         * @param partial_response R√©ponse partielle courante
         * @param is_finished true si la g√©n√©ration est termin√©e
         */
        public delegate void GenerationCallback(string partial_response, bool is_finished);

        /**
         * Met √† jour la configuration du timeout depuis les pr√©f√©rences
         */
        public void update_config() {
            config_manager.load();
            stderr.printf("[TRACE][OUT] MODELMANAGER: Configuration mise √† jour - timeout: %d sec\n",
                config_manager.get_generation_timeout());
        }

        /**
         * Destructeur - lib√®re les ressources
         */
        ~ModelManager() {
            unload_current_model();

            if (is_backend_initialized && !is_simulation_mode) {
                Llama.backend_free();
                is_backend_initialized = false;
            }
        }
    }
}
